import os
import requests
import numpy as np
import faiss
import re
from datetime import datetime
from sqlalchemy.orm import Session
from typing import List, Optional
from sentence_transformers import SentenceTransformer
from app.models import Trade

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π
from ..ai_modules.news_provider import ForexNewsProvider


class AI_Client:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è AI-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ—Ä–≥–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –∏ –Ω–æ–≤–æ—Å—Ç–µ–π
    """

    def __init__(self, db_session: Session):
        self.db = db_session
        self.api_key = os.getenv("OPENROUTER_API_KEY")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._initialize_ai_components()

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        self._load_and_index_trades()
        self._load_news_data()

    def _initialize_ai_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –º–æ–¥–µ–ª–µ–π –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
        print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI —Å–∏—Å—Ç–µ–º—ã...")
        self.embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
        self.dim = 384

        # FAISS –∏–Ω–¥–µ–∫—Å—ã
        self.trade_index = None
        self.news_index = None
        self.trade_texts = []
        self.news_texts = []
        self.all_trades = []

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API
        self.ai_model = "meta-llama/llama-3.3-70b-instruct:free"
        self.base_url = "https://openrouter.ai/api/v1"

    def _load_and_index_trades(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å–¥–µ–ª–æ–∫ –∏–∑ –ë–î"""
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Å–¥–µ–ª–æ–∫ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        self.all_trades = self.db.query(Trade).order_by(Trade.date.desc()).all()

        if not self.all_trades:
            print("‚ö†Ô∏è –í –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å–¥–µ–ª–∫–∏")
            return

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π —Å–¥–µ–ª–æ–∫
        self.trade_texts = [
            f"–°–î–ï–õ–ö–ê: –î–∞—Ç–∞={trade.date.strftime('%Y-%m-%d')}, "
            f"–°–∏–º–≤–æ–ª={trade.symbol}, –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ={trade.direction}, "
            f"R:R={trade.rr}, –ü—Ä–æ—Ñ–∏—Ç=${trade.profit}, –†–µ–∑—É–ª—å—Ç–∞—Ç={trade.result_type}, "
            f"–°–µ—Å—Å–∏—è={trade.session}, –ü–æ–∑–∏—Ü–∏—è={trade.position}, "
            f"–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π={trade.notes or '–Ω–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è'}"
            for trade in self.all_trades
        ]

        print(f"üìà –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.trade_texts)} —Å–¥–µ–ª–æ–∫")

        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è —Å–¥–µ–ª–æ–∫
        embeddings = [
            self.embedding_model.encode(text, convert_to_numpy=True).astype("float32")
            for text in self.trade_texts
        ]

        if embeddings:
            embeddings_array = np.vstack(embeddings)
            self.trade_index = faiss.IndexFlatL2(self.dim)
            self.trade_index.add(embeddings_array)
            print("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å–¥–µ–ª–æ–∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω")

    def _load_news_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ ForexNewsProvider"""
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            provider = ForexNewsProvider()

            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ (top_k=None)
            news_data = provider.get_latest_news(top_k=None)

            if not news_data:
                print("üì∞ –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
                return

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è AI
            self.news_texts = [
                f"–ù–û–í–û–°–¢–¨: –î–∞—Ç–∞={news.get('date')}, –ó–∞–≥–æ–ª–æ–≤–æ–∫={news.get('title')}, "
                f"–ò—Å—Ç–æ—á–Ω–∏–∫=ForexFactory, –í–∞–∂–Ω–æ—Å—Ç—å={news.get('impact')}, "
                f"–ü—Ä–æ–≥–Ω–æ–∑={news.get('forecast', '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö')}, "
                f"–ü—Ä–µ–¥—ã–¥—É—â–µ–µ={news.get('previous', '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö')}, "
                f"–§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ={news.get('actual', '–µ—â—ë –Ω–µ –≤—ã—à–ª–æ')}"
                for news in news_data
            ]

            # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ FAISS –∏–Ω–¥–µ–∫—Å–∞
            embeddings = [
                self.embedding_model.encode(text, convert_to_numpy=True).astype("float32")
                for text in self.news_texts
            ]

            if embeddings:
                embeddings_array = np.vstack(embeddings)
                self.news_index = faiss.IndexFlatL2(self.dim)
                self.news_index.add(embeddings_array)
                print(f"üì∞ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(news_data)} –Ω–æ–≤–æ—Å—Ç–µ–π")

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")

    def _extract_date_from_query(self, user_query: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        date_patterns = [
            r'(\d{4}\.\d{2}\.\d{2})',  # 2025.07.10
            r'(\d{1,2}\.\d{1,2}\.\d{4})',  # 10.07.2025
            r'(\d{1,2}\.\d{1,2})',  # 10.07
            r'(\d{4}-\d{2}-\d{2})',  # 2025-07-10
        ]

        for pattern in date_patterns:
            match = re.search(pattern, user_query)
            if match:
                return match.group(1)
        return None

    def _normalize_date(self, date_str: str) -> Optional[str]:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞—Ç—ã –∫ —Ñ–æ—Ä–º–∞—Ç—É YYYY-MM-DD"""
        date_formats = [
            '%Y.%m.%d',  # 2025.07.10
            '%d.%m.%Y',  # 10.07.2025
            '%Y-%m-%d',  # 2025-07-10
        ]

        # –î–ª—è –¥–∞—Ç –±–µ–∑ –≥–æ–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –≥–æ–¥
        if re.match(r'^\d{1,2}\.\d{1,2}$', date_str):
            date_str = f"{date_str}.{datetime.now().year}"
            date_formats.append('%d.%m.%Y')

        for fmt in date_formats:
            try:
                parsed_date = datetime.strptime(date_str, fmt)
                return parsed_date.strftime('%Y-%m-%d')
            except ValueError:
                continue
        return None

    def _find_trades_by_date(self, date_str: str) -> List[str]:
        """–ü–æ–∏—Å–∫ —Å–¥–µ–ª–æ–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –¥–∞—Ç–µ"""
        normalized_date = self._normalize_date(date_str)
        if not normalized_date:
            return []

        found_trades = []
        for trade_text in self.trade_texts:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è —Å–¥–µ–ª–∫–∏
            date_match = re.search(r'–î–∞—Ç–∞=([^,]+)', trade_text)
            if date_match and date_match.group(1) == normalized_date:
                found_trades.append(trade_text)

        return found_trades

    def _get_trade_count_from_query(self, user_query: str) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö —Å–¥–µ–ª–æ–∫"""
        user_query_lower = user_query.lower()

        # –ü–æ–∏—Å–∫ —á–∏—Å–ª–æ–≤—ã—Ö —É–∫–∞–∑–∞–Ω–∏–π
        numbers = re.findall(r'\d+', user_query)
        if numbers:
            return min(int(numbers[0]), 15)

        # –ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —É–∫–∞–∑–∞–Ω–∏–π
        count_mapping = {
            '–Ω–µ—Å–∫–æ–ª—å–∫–æ': 3, '–Ω–µ–º–Ω–æ–≥–æ': 3, '–ø–∞—Ä—É': 2,
            '–¥–µ—Å—è—Ç–æ–∫': 10, '–æ–∫–æ–ª–æ –¥–µ—Å—è—Ç–∏': 10,
            '–º–Ω–æ–≥–æ': 8, '–≤—Å–µ': min(15, len(self.trade_texts)),
            '–ø–æ–ª–Ω—ã–π': min(15, len(self.trade_texts))
        }

        for keyword, count in count_mapping.items():
            if keyword in user_query_lower:
                return count

        return 5  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    def _search_relevant_trades(self, query: str, top_k: int = 5) -> List[str]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å–¥–µ–ª–æ–∫"""
        if not self.trade_index or not self.trade_texts:
            return []

        try:
            query_embedding = self.embedding_model.encode(query, convert_to_numpy=True)
            query_embedding = query_embedding.astype("float32").reshape(1, -1)

            distances, indices = self.trade_index.search(query_embedding, top_k)

            return [
                self.trade_texts[idx] for idx in indices[0]
                if 0 <= idx < len(self.trade_texts)
            ]
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å–¥–µ–ª–æ–∫: {e}")
            return []

    def _search_relevant_news(self, query: str, top_k: int = 15) -> List[str]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        if not self.news_index or not self.news_texts:
            return []

        try:
            query_embedding = self.embedding_model.encode(query, convert_to_numpy=True)
            query_embedding = query_embedding.astype("float32").reshape(1, -1)

            distances, indices = self.news_index.search(query_embedding, top_k)

            return [
                self.news_texts[idx] for idx in indices[0]
                if 0 <= idx < len(self.news_texts)
            ]
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    def _get_latest_trades(self, n: int) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–¥–µ–ª–æ–∫ –ø–æ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏–∏"""
        return self.trade_texts[:n]

    def _classify_query_intent(self, user_query: str) -> dict:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        query_lower = user_query.lower()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥–∞—Ç—ã –≤ –∑–∞–ø—Ä–æ—Å–µ
        has_date = self._extract_date_from_query(user_query) is not None

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        intent_patterns = {
            'analysis': ['–∞–Ω–∞–ª–∏–∑', '–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä', '—Ä–∞–∑–±–æ—Ä', '—Ä–∞–∑–±–µ—Ä–∏', '–ø–æ—Å–º–æ—Ç—Ä–∏'],
            'psychology': ['–ø—Å–∏—Ö–æ–ª–æ–≥', '—ç–º–æ—Ü', '–¥–∏—Å—Ü–∏–ø–ª–∏–Ω', '–∂–∞–¥–Ω–æ—Å—Ç', '—Å—Ç—Ä–∞—Ö', 'fomo'],
            'mistakes': ['–æ—à–∏–±–∫', '–ø—Ä–æ–±–ª–µ–º', '–Ω–µ–ø—Ä–∞–≤', '–∏—Å–ø—Ä–∞–≤', '—É–ª—É—á—à'],
            'news': ['–Ω–æ–≤–æ—Å—Ç–∏', 'news', '—Å–æ–±—ã—Ç–∏—è', '—ç–∫–æ–Ω–æ–º–∏–∫–∞', '—Ä—ã–Ω–æ–∫', '—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª']
        }

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
        intent = "general"
        for intent_type, keywords in intent_patterns.items():
            if any(keyword in query_lower for keyword in keywords):
                intent = intent_type
                break

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
        needs_trades = any(word in query_lower for word in
                           ['—Å–¥–µ–ª–∫', '–ø–æ—Å–ª–µ–¥–Ω', '–Ω–µ–¥–∞–≤–Ω', '–º–æ–∏', '–∂—É—Ä–Ω–∞–ª', '–∏—Å—Ç–æ—Ä–∏']) or has_date

        needs_news = intent == "news" or any(word in query_lower for word in
                                             ['–Ω–æ–≤–æ—Å—Ç–∏', '—Å–æ–±—ã—Ç–∏—è', '—ç–∫–æ–Ω–æ–º–∏–∫–∞'])

        return {
            "intent": intent,
            "needs_trades": needs_trades,
            "needs_news": needs_news,
            "has_date": has_date,
            "is_general_question": not needs_trades and not needs_news and intent in ["psychology", "general"]
        }

    def _call_ai_api(self, prompt: str) -> str:
        """–í—ã–∑–æ–≤ –≤–Ω–µ—à–Ω–µ–≥–æ AI API"""
        if not self.api_key:
            return "‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç API –∫–ª—é—á –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ AI"

        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "http://localhost:5000",
                "X-Title": "Trade Analysis AI"
            }

            payload = {
                "model": self.ai_model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2000,
                "temperature": 0.7,
            }

            response = requests.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                headers=headers,
                timeout=60
            )

            if response.status_code != 200:
                error_msg = response.json().get('error', {}).get('message', 'Unknown error')
                raise Exception(f"API Error {response.status_code}: {error_msg}")

            data = response.json()
            return data["choices"][0]["message"]["content"].strip()

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ AI API: {e}")
            return f"‚ö†Ô∏è –í—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å AI —Å–µ—Ä–≤–∏—Å–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –ø–æ–∑–∂–µ."

    def _clean_response(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ AI"""
        # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r'[ \t]+', ' ', text)
        return text.strip()

    def _create_adaptive_prompt(self, user_query: str, trades: List[str], query_intent: dict) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–¥–µ–ª–æ–∫"""
        if not trades and query_intent["needs_trades"]:
            return self._create_no_data_prompt(user_query)

        trades_context = "\n".join([f"{i + 1}. {trade}" for i, trade in enumerate(trades)])

        # –ë–∞–∑–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI
        base_context = f"""
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å: "{user_query}"

–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–¥–µ–ª–æ–∫:
{trades_context}
"""

        # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤
        intent_instructions = {
            "analysis": "–ü—Ä–æ–≤–µ–¥–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–¥–µ–ª–æ–∫. –í—ã—è–≤–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∏ –∑–æ–Ω—ã —Ä–æ—Å—Ç–∞. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω –∏ –¥–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.",
            "psychology": "–°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö —Ç—Ä–µ–π–¥–∏–Ω–≥–∞. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ –¥–∞–π —Å–æ–≤–µ—Ç—ã –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏.",
            "mistakes": "–í—ã—è–≤–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –æ—à–∏–±–∫–∏ –∏ —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞. –ü—Ä–µ–¥–ª–æ–∂–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é –∏ —É–ª—É—á—à–µ–Ω–∏—é —Ç–æ—Ä–≥–æ–≤–æ–π –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã.",
            "general": "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–¥–µ–ª–æ–∫ –¥–ª—è –ø—Ä–∏–º–µ—Ä–æ–≤. –ë—É–¥—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º."
        }

        instruction = intent_instructions.get(query_intent["intent"], intent_instructions["general"])

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –¥–∞—Ç–∞–º–∏
        if query_intent["has_date"]:
            extracted_date = self._extract_date_from_query(user_query)
            instruction += f"\n\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–ø—Ä–æ—Å–∏–ª –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –∑–∞ –¥–∞—Ç—É {extracted_date}. –°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ —Å–¥–µ–ª–∫–∞—Ö –∑–∞ —ç—Ç—É –¥–∞—Ç—É –∏ –¥–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä –∏–º–µ–Ω–Ω–æ —ç—Ç–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π."

        prompt = f"""
–¢—ã - –æ–ø—ã—Ç–Ω—ã–π —Ç—Ä–µ–π–¥–µ—Ä-–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫ —Å –≥–ª—É–±–æ–∫–∏–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º —Ä—ã–Ω–∫–æ–≤ –∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏ —Ç—Ä–µ–π–¥–∏–Ω–≥–∞.

{base_context}

{instruction}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
- –ò—Å–ø–æ–ª—å–∑—É–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π —Ç–æ–Ω –æ–±—â–µ–Ω–∏—è
- –û–±—Ä–∞—â–∞–π—Å—è –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –Ω–∞ "—Ç—ã"
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω –∏ –ø—Ä–∞–∫—Ç–∏—á–µ–Ω
- –ò—Å–ø–æ–ª—å–∑—É–π 2-3 —ç–º–æ–¥–∑–∏ –¥–ª—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∞–∫—Ü–µ–Ω—Ç–æ–≤
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫–∏, –Ω–æ –±–µ–∑ –∂–µ—Å—Ç–∫–∏—Ö —à–∞–±–ª–æ–Ω–æ–≤
- –°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö insights
- –ü—Ä–µ–¥–ª–∞–≥–∞–π actionable —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

–û—Ç–≤–µ—Ç—å —Ç–∞–∫, –∫–∞–∫ –±—É–¥—Ç–æ –¥–∞–µ—à—å —Å–æ–≤–µ—Ç –∫–æ–ª–ª–µ–≥–µ-—Ç—Ä–µ–π–¥–µ—Ä—É.
"""
        return prompt

    def _create_news_prompt(self, user_query: str, news: List[str]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        if not news:
            return f"""
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–ø—Ä–æ—Å–∏–ª: "{user_query}"

–í –Ω–∞—Å—Ç–æ—è—â–∏–π –º–æ–º–µ–Ω—Ç –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.

–ü—Ä–µ–¥–ª–æ–∂–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é:
- –£—Ç–æ—á–Ω–∏—Ç—å –ø–µ—Ä–∏–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
- –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –æ —Ç–µ–∫—É—â–µ–π —Ä—ã–Ω–æ—á–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏
- –û–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –¥—Ä—É–≥–∏–º –∞—Å–ø–µ–∫—Ç–∞–º —Ç—Ä–µ–π–¥–∏–Ω–≥–∞
"""

        news_context = "\n".join([f"{i + 1}. {item}" for i, item in enumerate(news)])

        return f"""
–¢—ã - –æ–ø—ã—Ç–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–ø—Ä–æ—Å–∏–ª: "{user_query}"

–ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
{news_context}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ –∏:
1. –í—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ —Ä—ã–Ω–∫–∏
2. –û—Ü–µ–Ω–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã–µ –∞–∫—Ç–∏–≤—ã (–∞–∫—Ü–∏–∏, –≤–∞–ª—é—Ç—ã, –∏–Ω–¥–µ–∫—Å—ã)
3. –î–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Ç—Ä–µ–π–¥–µ—Ä–æ–≤
4. –£–∫–∞–∂–∏ –Ω–∞ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∏—Å–∫–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

–ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω –∏ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
"""

    def _create_no_data_prompt(self, user_query: str) -> str:
        """–ü—Ä–æ–º–ø—Ç –¥–ª—è —Å–ª—É—á–∞–µ–≤ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        return f"""
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–ø—Ä–æ—Å–∏–ª: "{user_query}"

–í –Ω–∞—Å—Ç–æ—è—â–∏–π –º–æ–º–µ–Ω—Ç –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å–¥–µ–ª–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.

–í–µ–∂–ª–∏–≤–æ —Å–æ–æ–±—â–∏ –æ–± —ç—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –∏ –ø—Ä–µ–¥–ª–æ–∂–∏:
- –î–æ–±–∞–≤–∏—Ç—å —Å–¥–µ–ª–∫–∏ –≤ —Ç–æ—Ä–≥–æ–≤—ã–π –∂—É—Ä–Ω–∞–ª
- –£—Ç–æ—á–Ω–∏—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø–æ–∏—Å–∫–∞
- –ó–∞–¥–∞—Ç—å –æ–±—â–∏–π –≤–æ–ø—Ä–æ—Å –æ —Ç—Ä–µ–π–¥–∏–Ω–≥–µ

–ë—É–¥—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–º –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–º–æ—â–∏.
"""

    def analyze(self, user_query: str) -> str:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ (—Å–¥–µ–ª–∫–∏ + –Ω–æ–≤–æ—Å—Ç–∏)
        """
        print(f"üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: '{user_query}'")

        # –ê–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        query_intent = self._classify_query_intent(user_query)
        print(f"üîç –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ: {query_intent['intent']}")

        # –í—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–Ω–∞–ª–∏–∑–∞
        if query_intent["needs_news"]:
            print("üì∞ –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π...")
            relevant_news = self._search_relevant_news(user_query, top_k=15)
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(relevant_news)}")
            prompt = self._create_news_prompt(user_query, relevant_news)
        else:
            # –ê–Ω–∞–ª–∏–∑ —Å–¥–µ–ª–æ–∫
            relevant_trades = self._find_relevant_trades(user_query, query_intent)
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å–¥–µ–ª–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(relevant_trades)}")
            prompt = self._create_adaptive_prompt(user_query, relevant_trades, query_intent)

        print("üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è AI –æ—Ç–≤–µ—Ç–∞...")
        response = self._call_ai_api(prompt)
        return self._clean_response(response)

    def _find_relevant_trades(self, user_query: str, query_intent: dict) -> List[str]:
        """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å–¥–µ–ª–æ–∫"""

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –¥–∞—Ç–µ
        if query_intent["has_date"]:
            extracted_date = self._extract_date_from_query(user_query)
            if extracted_date:
                date_trades = self._find_trades_by_date(extracted_date)
                if date_trades:
                    print(f"üìÖ –ù–∞–π–¥–µ–Ω–æ —Å–¥–µ–ª–æ–∫ –ø–æ –¥–∞—Ç–µ {extracted_date}: {len(date_trades)}")
                    return date_trades
                else:
                    print(f"üìÖ –°–¥–µ–ª–æ–∫ –ø–æ –¥–∞—Ç–µ {extracted_date} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫")

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞
        if query_intent["needs_trades"]:
            trade_count = self._get_trade_count_from_query(user_query)

            if any(word in user_query.lower() for word in ['–ø–æ—Å–ª–µ–¥–Ω', '–Ω–µ–¥–∞–≤–Ω']):
                return self._get_latest_trades(trade_count)
            else:
                return self._search_relevant_trades(user_query, trade_count)

        return []  # –î–ª—è –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ —Ç—Ä–µ–±—É—é—Ç—Å—è —Å–¥–µ–ª–∫–∏